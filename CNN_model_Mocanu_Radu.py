# -*- coding: utf-8 -*-
"""BrainScanCNNUnderSampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_rwQPLs33xulcsUZMcjMXjCGjz00RuH4
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip  gdrive/My\ Drive/archive.zip

import numpy as np 
from tensorflow.keras.losses import SparseCategoricalCrossentropy
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from PIL import Image
from tensorflow.keras import datasets, layers, models 
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras
import cv2
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import ModelCheckpoint
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix
import seaborn as sns
import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_curve

def get_average_predictions(files_list):
  val_list = [0 for i in range (0, 5149)]
  new_predict=[None for i in range (0, 5149)]
  for filename in files_list:
     with open(filename, 'r') as f:
        f.readline()
        for i in range (0, 5149):
          number,prediction = [int(x) for x in f.readline().split(",")]
          if prediction == 0:
            val_list[i] -= 1
          else:
            val_list[i] += 1
     f.close()
  for index in range (0, 5149):
    print (val_list[index])
    if val_list[index] < 0:
      new_predict[index] = 0
    else:
      new_predict[index] = 1
  return new_predict

# adauga zerouri imaginii astfel incat sa se potriveasca cu formatul provided
def add_zeros(image_number):
    zeros_to_add = 6 - len(image_number)
    zeros = zeros_to_add * "0"
    return zeros + image_number

# intoarce imaginea taiata si normalizata (pixeli intre 0 si 1)
def normalize_and_cut_image(image):
    # conversie la np array de tip float 32
    img_float32 = np.float32(image)
    # tai marginile negre din imagine  
    img_float32 = img_float32[50:170]
    img_float32 = img_float32[:,50:170]
    # normalizez in range-ul 0-1
    img_normalized = cv2.normalize(img_float32, None, 0, 1.0, cv2.NORM_MINMAX)
    return img_normalized

# citeste imaginile din fisierul dezarhivat
def get_images(start, number_of_images, base_path, number_of_images_to_process_at_a_time):
    # lista ce va contine imaginile de la iteratia curenta
    processed_images = []
    # lista ce va contine imaginile de la iteratia curenta
    total_images = []
    for i in range(start, start + number_of_images):
        image_name = add_zeros(str(i + 1))
         # convertesc imaginea din rgb in alb negru 
        img = Image.open(base_path + image_name + '.png').convert('L')
        # adaug imaginea procesata 
        processed_images.append(normalize_and_cut_image(img))
        if i % number_of_images_to_process_at_a_time == 0:
           #  adaug doar un numar prestabilit de imagini la un moment dat pentru a nu da exceed memoria RAM
          total_images = total_images + processed_images
          # golesc lista 
          processed_images = []
          
   
    # verific daca am ajuns la finalul imaginilor de citit
    if len(processed_images):
      return total_images + processed_images
    return  total_images

# creeaza fisierul de predictii in format csv 
 def createCsvFile(csvFileName, predictions): 
  with open(csvFileName, "w") as f: 
    print("id,class", file =f)
    predict_index = -1
    for i in range (17001, 22150):
      predict_index += 1
      print("0" + str(int(i)) + "," + str(int(predictions[predict_index])), file=f)
  f.close()

# citeste labelurile din fisier
def get_labels(number_of_rows, path):
  labels = []
  fisier = open(path ,'r')
  fisier.readline()
  for i in range (number_of_rows):
    img_number, label = [int (i) for i in fisier.readline().split(",")]
    labels.append(label)
  fisier.close()
  return labels

max_f1_score = 0

# creez checkpointul custom
class ConfusionMatrixCheckpoint(ModelCheckpoint):
    # argumentul required al constructorului va fi doar path-ul unde va fi salvat modelul
     def __init__(self, filepath, monitor='val_confusion_matrix', verbose=0, save_best_only=True, mode='min', save_weights_only=False):
        super().__init__(filepath, monitor=monitor, verbose=verbose, save_best_only=save_best_only, mode=mode, save_weights_only=save_weights_only)
        # ma asigur ca valoarea lui confusion_matrix nu a fost preinitializata
        self.confusion_matrix = None
    # metoda utilizata in versiunile mai vechi pentru a clasifica mai multe imagini ca fiind 1 
     def get_bias_class_predict(self, predictions):
      bias_predict = []
      for pred in predictions:
        if pred[1] >= 0.4:
            bias_predict.append(1)
        else:
            bias_predict.append(0)
      return bias_predict
    # suprascriu comportamentul metodei on_epoch_end 
     def on_epoch_end(self, epoch, logs=None):
       global max_f1_score
       predictions = self.model.predict(validation_images)
       accuracy =self.model.evaluate(validation_images, validation_labels)[1]
       # uncomment for bias predict
       # conf_matrix = confusion_matrix([np.argmax(pred) for pred in validation_labels], self.get_bias_class_predict(predictions))
       # modific predictiile si labelurile intr-un format acceptat de confusion_matrix 
       classes_predictions=[np.argmax(var) for var in predictions]
       val_labels = [np.argmax(lab) for lab in validation_labels]
       #calculez si afisez matricea de confuzie de la epoca curenta 
       conf_matrix = confusion_matrix(val_labels, classes_predictions)
       print(conf_matrix)
       #calculez scorul f1 curent
       current_f1 = f1_score(val_labels,classes_predictions)
       # daca scorul curent e cel mai mare de pana la momentul actual, suprascriu cel mai bun model salvat
       if current_f1 > max_f1_score:
         max_f1_score = current_f1
         self.model.save(self.filepath, overwrite=True)
         print(f'Saved model at epoch {epoch+1} with f1_score={max_f1_score}')

# creez o instanta a checkpointului custom 
conf_matrix_checkpoint = ConfusionMatrixCheckpoint('/content/saved_models/best_model.h5')

# intoarce matricea de confuzie 
def getConfusionMatrix(validation_labels, classes_predictions):
  conf_matrix = confusion_matrix(validation_labels, classes_predictions)
  return conf_matrix

# intoarce valoarea recall-ului pe clasa specificata
def getRecallValue(conf_matrix, classNumber):
  if classNumber == 0:
    return conf_matrix[0][0]/ (conf_matrix[0][0] + conf_matrix[0][1])
  return conf_matrix[1][1]/ (conf_matrix[1][1] + conf_matrix[1][0])

# intoarce valoarea preciziei pe clasa specificata
def getPrecisionValue(conf_matrix, classNumber):
  if classNumber == 0:
    return conf_matrix[0][0]/ (conf_matrix[0][0] + conf_matrix[1][0])
  return conf_matrix[1][1]/ (conf_matrix[1][1] + conf_matrix[0][1])

# intoarce valoarea acuratetei
def getAccuracyValue(conf_matrix):
  return (conf_matrix[0][0] + conf_matrix[1][1])/np.sum(conf_matrix)

# creeaza modelul cnn
# desi are un numar mare de straturi, datorita layerelor de dropout si batchNormalization modelul nu face overfitting
def create_cnn_model_to_train():
    
    model = models.Sequential()
    model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(120, 120, 1)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(16, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(16, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(16, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(8, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(4, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))
    

    return model

# creez o instanta a modelului
cnnModel = create_cnn_model_to_train()
# definesc learnig rate custom 
adaptable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)
# definesc un callback de early_stop 
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)

# compilez modelul cu un learning_rate mai mare, deoarece acesta va scadea overtime 
  cnnModel.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.0007),
                loss=tf.keras.losses.BinaryCrossentropy(),
                metrics=['accuracy', tf.keras.metrics.AUC()])

# citesc imaginile 
train_images  = get_images(0, 15000, '/content/data/data/', 500)

validation_images = get_images(15000, 2000, '/content/data/data/', 500)

test_images = get_images(17000, 5149, '/content/data/data/', 500)

# citesc labelurile 
train_labels = get_labels(15000, '/content/train_labels.txt')

validation_labels = get_labels(2000, '/content/validation_labels.txt')

# le covertesc la np.array
validation_images = np.array(validation_images)
validation_labels = np.array(validation_labels)
test_images = np.array(test_images)
train_images = np.array(train_images)
train_labels = np.array(train_labels)

# definesc un pipeline pt oversampling si undersampling 
add_minority_samples = SMOTE(random_state=38,sampling_strategy=0.8)
delete_majority_samples = RandomUnderSampler()
sampling_strategy = [('o', add_minority_samples), ('u', delete_majority_samples)]
apply_sampling = Pipeline(steps=sampling_strategy)

# redimensionez imaginile într-un array unidimensional pentru a putea fi utilizat cu SMOTE
train_images_reshaped = train_images.reshape((-1, 120*120))

# aplic SMOTE pe setul de date
train_images, train_labels = apply_sampling.fit_resample(train_images_reshaped, train_labels)


# redimensionez imaginile înapoi în forma inițială
train_images = train_images.reshape((-1, 120, 120))

# plotez imagini pt a le analiza + labelul ei 

idx =8902
print(train_labels[idx])
image = train_images[idx]
plt.imshow(image, cmap='gray')
plt.show()

# verific daca pipeline ul de over si undersampling a functionat
from collections import Counter
print(Counter(train_labels))

#convertesc labelurile in one-hot encoding

train_labels = tf.keras.utils.to_categorical(train_labels)
validation_labels = tf.keras.utils.to_categorical(validation_labels)

# antrenez modelul folosind callbackurile definite mai sus 
fit_details = cnnModel.fit(train_images, train_labels, epochs=100, callbacks=[adaptable_learning_rate, early_stop,conf_matrix_checkpoint]
                      validation_data=(validation_images,validation_labels), shuffle=True)

#calculez matricea de confuzie 
conf_matrix = getConfusionMatrix([np.argmax(pred) for pred in validation_labels],classes_predictions)
print(conf_matrix)

# ploteaza matricea de confuzie
sns.heatmap(conf_matrix, cmap="YlOrRd", annot=True, fmt='d')
plt.title('Matricea de confuzie')
plt.show()

# ploteaza precizia fiecarei clase
precisions = [getPrecisionValue(conf_matrix, c) for c in [0,1]]
plt.bar(["Clasa 0 ", "Clasa 1"], precisions)
plt.xlabel('Clase')
plt.ylabel('Precizie')
plt.title('Precizia in functie de clasa')
plt.show()

#ploteaza acuratetea modelului + a datelor de validare, in functie de epoca

plt.plot(fit_details.history['accuracy'])
plt.plot(fit_details.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# ploteaza recall-ul fiecarei clase
recalls = [getRecallValue(conf_matrix, c) for c in [0,1]]
plt.bar(["Clasa 0 ", "Clasa 1"], recalls, color="red")
plt.xlabel('Clase')
plt.ylabel('Recall')
plt.title('Recall-ul in functie de clasa')
plt.show()

print(getAccuracyValue(conf_matrix))

# incarc cea mai buna stare a modelului ( cel mai bun f1 score)
cnnModel.load_weights('/content/saved_models/best_model.h5')

# aflu predictiile modelului 
predictions=cnnModel.predict(validation_images) 
classes_predictions=[np.argmax(var) for var in predictions]

# exemplu de apelare a metodei de average predictions 
classes_predictions = get_average_predictions(["prediction_output_669.csv","prediction_output_627.csv","prediction_output_600.csv","prediction_output_519.csv"])

# aflu cati de 1 sunt in oredictii 
print(np.count_nonzero(classes_predictions))

# creez fisierul csv 
createCsvFile("submission_635.csv",classes_predictions)